# Tutoriel Mini-RAG - CFTL JTIA 2025 Paris

Application Flask pour le tutoriel de construction d'un pipeline RAG (Retrieval-Augmented Generation) avec un assistant testeur ISTQB certifiÃ©.

## ğŸ¯ Description

Cette application implÃ©mente un pipeline RAG complet permettant de :
- **Indexer** des documents (PDF, DOCX, TXT, MD) avec FAISS
- **Rechercher** dans les documents via similaritÃ© vectorielle
- **GÃ©nÃ©rer** des rÃ©ponses contextuelles avec un LLM
- **Assister** dans les activitÃ©s de test selon les standards ISTQB

L'assistant IA se comporte comme un **testeur certifiÃ© ISTQB** capable d'assister dans :
- ğŸ“‹ **Analyse des tests** : Identifier les conditions de test
- ğŸ¯ **Conception des tests** : CrÃ©er des cas de test dÃ©taillÃ©s
- âš™ï¸ **ImplÃ©mentation des tests** : PrÃ©parer scripts et environnement
- â–¶ï¸ **ExÃ©cution des tests** : DÃ©finir procÃ©dures et critÃ¨res

## ğŸ”§ Modes de fonctionnement

L'application supporte deux modes configurables via le fichier `.env` :

### Mode OpenAI (par dÃ©faut)
- Embeddings : OpenAI text-embedding-3-small
- LLM : OpenAI GPT (configurable : gpt-4o-mini, gpt-4o, etc.)
- ClÃ© API fournie pour l'atelier

### Mode Local
- Embeddings : Sentence Transformers (paraphrase-multilingual-MiniLM-L12-v2)
- LLM : Ollama (llama3.2:3b ou autre modÃ¨le)
- Fonctionne 100% en local sans API externe

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8 ou supÃ©rieur
- pip

### Ã‰tapes d'installation

1. **Activer votre environnement virtuel** (si ce n'est pas dÃ©jÃ  fait) :

**Windows (cmd.exe) :**
```cmd
venv\Scripts\activate
```

**Windows (PowerShell) :**
```powershell
venv\Scripts\Activate.ps1
```

**Linux/Mac :**
```bash
source venv/bin/activate
```

2. **Installer les dÃ©pendances :**
```cmd
pip install -r requirements.txt
```

### Configuration

CrÃ©er un fichier `.env` Ã  la racine du projet :

```env
# Configuration Flask
FLASK_APP=app.py
FLASK_ENV=development
FLASK_DEBUG=1

# ClÃ© API OpenAI (fournie pour l'atelier)
OPENAI_API_KEY=votre_clÃ©_api

# Mode de fonctionnement : 'openai' ou 'local'
EMBEDDING_MODE=openai
LLM_MODE=openai

# ModÃ¨les
OLLAMA_MODEL=llama3.2:3b
OPENAI_MODEL=gpt-4o
```

### Installation du mode local (optionnel)

Pour utiliser le mode local, installer Ollama :
- **Windows/Mac** : https://ollama.com/download
- **Linux** : `curl -fsSL https://ollama.com/install.sh | sh`

Puis tÃ©lÃ©charger un modÃ¨le :
```bash
ollama pull llama3.2:3b
```

## â–¶ï¸ Lancement de l'application

```cmd
python app.py
```

ou

```cmd
flask run
```

L'application sera accessible sur : http://localhost:5000

## ğŸ“ Structure du projet

```
tuto_mini-rag/
â”œâ”€â”€ app.py                      # Application Flask principale
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ .env                        # Variables d'environnement (non versionnÃ©)
â”œâ”€â”€ modules/                    # Modules RAG
â”‚   â”œâ”€â”€ document_processor.py  # Extraction de texte
â”‚   â”œâ”€â”€ chunker.py             # DÃ©coupage en chunks
â”‚   â”œâ”€â”€ indexer.py             # Indexation FAISS
â”‚   â”œâ”€â”€ local_embedder.py      # Embeddings locaux
â”‚   â””â”€â”€ local_llm.py           # LLM local (Ollama)
â”œâ”€â”€ templates/                  # Templates HTML
â”‚   â”œâ”€â”€ index.html             # Page d'accueil
â”‚   â”œâ”€â”€ upload.html            # Upload de documents
â”‚   â”œâ”€â”€ indexation.html        # Indexation FAISS
â”‚   â”œâ”€â”€ search.html            # Recherche et chat
â”‚   â””â”€â”€ navigation.html        # Menu de navigation
â”œâ”€â”€ static/                     # Fichiers statiques
â”‚   â”œâ”€â”€ css/                   # Styles CSS
â”‚   â””â”€â”€ js/                    # Scripts JavaScript
â”œâ”€â”€ uploads/                    # Documents uploadÃ©s
â””â”€â”€ data/                       # Index FAISS et mÃ©tadonnÃ©es
```

## ğŸ“ Utilisation

### Ã‰tape 1 : Upload de documents
- AccÃ©dez Ã  la page "Upload"
- Glissez-dÃ©posez vos documents (PDF, DOCX, TXT, MD)
- Les formats supportÃ©s : PDF, Word, Texte, Markdown

### Ã‰tape 2 : Indexation
- Allez sur la page "Indexation"
- Configurez les paramÃ¨tres (chunk size, overlap)
- Lancez l'indexation FAISS
- L'index vectoriel sera crÃ©Ã© automatiquement

### Ã‰tape 3 : Recherche et gÃ©nÃ©ration
- Page "Utiliser" pour interroger vos documents
- Posez vos questions en langage naturel
- L'assistant testeur ISTQB rÃ©pond en se basant sur vos documents
- Historique de conversation conservÃ© pendant la session

### Utilisation via API (cURL)

Vous pouvez Ã©galement interroger le systÃ¨me directement via l'API REST :

```bash
curl -X POST http://localhost:5000/api/search \
  -H "Content-Type: application/json" \
  -d "{\"question\": \"Quels sont les principes de base du test logiciel selon ISTQB?\", \"top_k\": 5, \"temperature\": 0.7, \"max_tokens\": 500}"
```

**ParamÃ¨tres :**
- `question` (requis) : Votre question en langage naturel
- `top_k` (optionnel) : Nombre de chunks Ã  rÃ©cupÃ©rer (dÃ©faut: 5)
- `temperature` (optionnel) : CrÃ©ativitÃ© du LLM 0-1 (dÃ©faut: 0.7)
- `max_tokens` (optionnel) : Longueur max de la rÃ©ponse (dÃ©faut: 500)

**RÃ©ponse JSON :**
```json
{
  "response": "RÃ©ponse gÃ©nÃ©rÃ©e par le LLM...",
  "sources": [
    {
      "text": "Texte du chunk...",
      "source": "document.pdf",
      "chunk_id": 0
    }
  ],
  "num_chunks": 5
}
```

## ğŸ“¦ DÃ©pendances principales

### Core
- `flask==3.0.0` - Framework web
- `python-dotenv==1.0.0` - Variables d'environnement

### Traitement de documents
- `PyPDF2==3.0.1` - Extraction PDF
- `python-docx==1.1.0` - Extraction Word
- `markdown==3.5.1` - Extraction Markdown

### RAG et embeddings
- `openai>=1.40.0` - Client OpenAI officiel
- `faiss-cpu==1.7.4` - Recherche de similaritÃ© vectorielle
- `tiktoken==0.5.1` - Comptage de tokens

### Mode local (optionnel)
- `sentence-transformers==3.3.1` - Embeddings locaux
- `ollama==0.6.1` - Client LLM local
- `torch==2.5.1` - Support PyTorch

## ğŸ”‘ FonctionnalitÃ©s

### Pipeline RAG complet
- âœ… Upload de documents multiples formats
- âœ… Chunking intelligent avec overlap
- âœ… Vectorisation (OpenAI ou local)
- âœ… Index FAISS pour recherche rapide
- âœ… Retrieval contextuel par similaritÃ©
- âœ… GÃ©nÃ©ration de rÃ©ponses avec LLM
- âœ… Historique de conversation

### Assistant testeur ISTQB
- âœ… Expertise en test logiciel
- âœ… Analyse de spÃ©cifications
- âœ… CrÃ©ation de cas de test
- âœ… StratÃ©gies de test
- âœ… Rapports professionnels

### Interface utilisateur
- âœ… Interface web moderne et responsive
- âœ… Navigation par Ã©tapes guidÃ©e
- âœ… Configuration visible en temps rÃ©el
- âœ… ParamÃ¨tres ajustables (top_k, tempÃ©rature, max_tokens)
- âœ… Affichage des sources utilisÃ©es

## ğŸ¯ Concepts couverts

- **Chunking** : DÃ©coupage intelligent de documents
- **Vectorisation** : Transformation en embeddings
- **Retrieval** : Recherche par similaritÃ© vectorielle
- **Assemblage de prompts** : Construction de contexte
- **GÃ©nÃ©ration contrÃ´lÃ©e** : LLM avec instructions
- **RÃ©duction des hallucinations** : RAG basÃ© sur documents

## ğŸ“¦ Stockage des Embeddings

Les vecteurs d'embeddings gÃ©nÃ©rÃ©s sont stockÃ©s localement dans le dossier `data/` :

### Fichiers d'index

1. **`data/faiss_index.bin`**
   - Contient l'index FAISS avec tous les vecteurs d'embeddings
   - C'est ici que FAISS effectue ses recherches ultra-rapides de similaritÃ©
   - Format binaire optimisÃ© pour les performances

2. **`data/index_metadata.pkl`**
   - Contient les mÃ©tadonnÃ©es associÃ©es aux vecteurs
   - Texte original des chunks
   - Sources des documents
   - Informations de traÃ§abilitÃ© (chunk_id, tokens, etc.)

### Processus d'indexation

1. DÃ©coupage des documents en chunks
2. GÃ©nÃ©ration d'un embedding (vecteur) pour chaque chunk via OpenAI ou Sentence Transformers
3. Stockage de tous les vecteurs dans l'index FAISS (`faiss_index.bin`)
4. Sauvegarde des textes originaux et mÃ©tadonnÃ©es (`index_metadata.pkl`)

### Processus de recherche

1. Transformation de votre question en vecteur
2. FAISS recherche les vecteurs les plus similaires dans l'index
3. RÃ©cupÃ©ration des chunks correspondants depuis les mÃ©tadonnÃ©es
4. GÃ©nÃ©ration de la rÃ©ponse par le LLM basÃ©e sur ces chunks

> ğŸ’¡ **Persistance** : Ces fichiers persistent entre les sessions - vous pouvez fermer l'application et l'index sera automatiquement rechargÃ© au redÃ©marrage.

## ğŸ“ Configuration avancÃ©e

### ParamÃ¨tres de chunking
- `chunk_size` : Taille des chunks (dÃ©faut: 500 tokens)
- `chunk_overlap` : Chevauchement (dÃ©faut: 50 tokens)

### ParamÃ¨tres de recherche
- `top_k` : Nombre de chunks Ã  rÃ©cupÃ©rer (dÃ©faut: 5)
- `temperature` : CrÃ©ativitÃ© du LLM (0-1, dÃ©faut: 0.7)
- `max_tokens` : Longueur maximale de la rÃ©ponse (dÃ©faut: 500)

---

Â© 2025 CFTL JTIA - Paris
