<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tutoriel Mini-RAG - CFTL JTIA 2025</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <header>
            <div class="logo-section">
                <h1>üöÄ Mini-RAG Pipeline</h1>
                <p class="subtitle">CFTL JTIA 2025 - Paris</p>
            </div>
            {% include 'navigation.html' %}
            
            <!-- Configuration Info -->
            <div class="config-info" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px 20px; border-radius: 8px; margin-top: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <div style="display: flex; align-items: center; gap: 15px; flex-wrap: wrap;">
                    <span style="font-weight: bold; font-size: 1.1em;">üîß Configuration:</span>
                    <div style="display: flex; gap: 20px; flex-wrap: wrap; flex: 1;">
                        <div style="background: rgba(255,255,255,0.2); padding: 8px 15px; border-radius: 5px;">
                            <strong>Embeddings:</strong> 
                            {% if embedding_mode == 'local' %}
                                üè° Local (Sentence Transformers)
                            {% else %}
                                ‚òÅÔ∏è OpenAI
                            {% endif %}
                        </div>
                        <div style="background: rgba(255,255,255,0.2); padding: 8px 15px; border-radius: 5px;">
                            <strong>LLM:</strong> 
                            {% if llm_mode == 'local' %}
                                üè° Local ({{ ollama_model }})
                            {% else %}
                                ‚òÅÔ∏è OpenAI ({{ openai_model }})
                            {% endif %}
                        </div>
                    </div>
                </div>
            </div>
        </header>

        <main id="top">
            <nav class="page-navigation">
                <h3>üìë Navigation</h3>
                <ul>
                    <li><a href="#description">üìã Description du Tutoriel</a></li>
                    <li><a href="#openai">ü§ñ OpenAI Embeddings</a></li>
                    <li><a href="#faiss">üóÑÔ∏è FAISS</a></li>
                    <li><a href="#fonctionnement">‚öôÔ∏è Comment √ßa Fonctionne</a></li>
                    <li><a href="#objectifs">üéØ Objectifs d'Apprentissage</a></li>
                    <li><a href="#stack">üõ†Ô∏è Stack Technique</a></li>
                    <li><a href="#prerequis">‚úÖ Pr√©requis</a></li>
                    <li><a href="#confidentialite">üîí Confidentialit√©</a></li>
                    <li><a href="#local">üè° Mode Local</a></li>
                </ul>
            </nav>

            <a href="#top" class="back-to-top" title="Retour au d√©but">
                ‚¨ÜÔ∏è
            </a>

            <section class="welcome-section">

                <h2>Bienvenue au Tutoriel</h2>
                <p class="intro">
                    D√©couvrez comment construire un pipeline RAG (Retrieval-Augmented Generation) 
                    pour g√©n√©rer automatiquement des cas de test et des rapports QA.
                </p>

                <section class="cta-section">
                    <div class="cta-box">
                        <h3>Pr√™t √† commencer ?</h3>
                        <p>Suivez les √©tapes du tutoriel pour construire votre propre pipeline RAG</p>
                        <a href="{{ url_for('upload') }}" class="cta-button">
                            D√©marrer le Tutoriel
                        </a>
                    </div>
                </section>


               
            </section>

            <section class="description-section" id="description">
                <h3>üìã Description du Tutoriel</h3>
                <div class="description-content">
                    <p>
                        Les participants construiront, <strong>en local avec VS Code</strong>, un mini-pipeline RAG 
                        combinant <strong>OpenAI Embeddings</strong> et <strong>FAISS</strong> pour indexer un corpus 
                        (sp√©cifications, user stories, tickets, ‚Ä¶), interroger efficacement les documents, 
                        puis g√©n√©rer automatiquement des cas de test ainsi qu'un rapport QA.
                    </p>
                    
                    <p>
                        Le parcours couvre le <strong>chunking</strong>, la <strong>vectorisation</strong>, 
                        le <strong>retrieval</strong>, l'assemblage de prompts et la g√©n√©ration contr√¥l√©e, 
                        avec un accent sur la qualit√© des sources et la r√©duction des hallucinations.
                    </p>
                    
                    <p>
                        √Ä l'issue du tutoriel, chaque participant repart avec un <strong>starter fonctionnel</strong> 
                        (scripts Python, prompts r√©utilisables) et des pistes d'industrialisation : 
                        √©valuation, gouvernance du corpus, int√©gration CI/CD.
                    </p>
                </div>
            </section>

            <section class="tech-explanation" id="openai">
                <h3>ü§ñ OpenAI Embeddings</h3>
                <div class="description-content">
                    <p>
                        Les <strong>embeddings OpenAI</strong> sont des repr√©sentations vectorielles de texte qui capturent 
                        le sens s√©mantique des mots et phrases. Le mod√®le <strong>text-embedding-3-small</strong> transforme 
                        chaque morceau de texte en un vecteur de <strong>1536 dimensions</strong>.
                    </p>
                    
                    <p>
                        Ces vecteurs permettent de mesurer la <strong>similarit√© s√©mantique</strong> entre deux textes : 
                        des textes qui ont un sens proche auront des vecteurs proches dans l'espace vectoriel, 
                        m√™me s'ils n'utilisent pas les m√™mes mots exacts.
                    </p>
                    
                    <p>
                        <strong>Avantage cl√© :</strong> Contrairement √† une recherche par mots-cl√©s traditionnelle, 
                        les embeddings comprennent le contexte et les nuances du langage naturel.
                    </p>
                </div>
            </section>

            <section class="tech-explanation" id="faiss">
                <h3>üóÑÔ∏è FAISS (Facebook AI Similarity Search)</h3>
                <div class="description-content">
                    <p>
                        <strong>FAISS</strong> est une biblioth√®que d√©velopp√©e par Meta (Facebook) qui permet de 
                        rechercher efficacement des vecteurs similaires parmi des millions d'embeddings. 
                        Elle utilise des <strong>algorithmes optimis√©s</strong> pour effectuer des recherches 
                        de similarit√© tr√®s rapidement.
                    </p>
                    
                    <p>
                        Dans notre pipeline RAG, FAISS <strong>indexe tous les chunks de documents</strong> sous forme 
                        de vecteurs. Quand vous posez une question, celle-ci est aussi transform√©e en vecteur, 
                        puis FAISS trouve instantan√©ment les <strong>k documents les plus similaires</strong> 
                        (recherche par k-NN : k plus proches voisins).
                    </p>
                    
                    <p>
                        <strong>Avantage cl√© :</strong> Performances exceptionnelles m√™me avec de grandes quantit√©s 
                        de documents, et fonctionne enti√®rement en local sans n√©cessiter de base de donn√©es externe.
                    </p>

                    <h4 style="margin-top: 30px;">üì¶ Stockage des Embeddings</h4>
                    <p>
                        Les vecteurs d'embeddings sont stock√©s dans le dossier <code>data/</code> de l'application :
                    </p>
                    <ul>
                        <li>
                            <strong><code>data/faiss_index.bin</code></strong> - Contient l'index FAISS avec tous les vecteurs d'embeddings. 
                            C'est ici que FAISS effectue ses recherches ultra-rapides de similarit√©.
                        </li>
                        <li>
                            <strong><code>data/index_metadata.pkl</code></strong> - Contient les m√©tadonn√©es associ√©es : 
                            texte original des chunks, sources des documents, et informations de tra√ßabilit√©.
                        </li>
                    </ul>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin-top: 15px;">
                        <p><strong>üîÑ Processus d'indexation :</strong></p>
                        <ol>
                            <li>D√©coupage des documents en chunks</li>
                            <li>G√©n√©ration d'un embedding (vecteur) pour chaque chunk via OpenAI ou Sentence Transformers</li>
                            <li>Stockage de tous les vecteurs dans l'index FAISS (<code>faiss_index.bin</code>)</li>
                            <li>Sauvegarde des textes originaux et m√©tadonn√©es (<code>index_metadata.pkl</code>)</li>
                        </ol>
                        
                        <p style="margin-top: 10px;"><strong>üîç Processus de recherche :</strong></p>
                        <ol>
                            <li>Transformation de votre question en vecteur</li>
                            <li>FAISS recherche les vecteurs les plus similaires dans l'index</li>
                            <li>R√©cup√©ration des chunks correspondants depuis les m√©tadonn√©es</li>
                            <li>G√©n√©ration de la r√©ponse par le LLM bas√©e sur ces chunks</li>
                        </ol>
                    </div>
                    
                    <p style="margin-top: 15px;">
                        üí° <strong>Persistance :</strong> Ces fichiers persistent entre les sessions - vous pouvez fermer 
                        l'application et l'index sera automatiquement recharg√© au red√©marrage.
                    </p>
                </div>
            </section>

            <section class="tech-explanation" id="fonctionnement">
                <h3>‚öôÔ∏è Comment cette Application Fonctionne</h3>
                <div class="description-content">
                    <p>
                        Cette application impl√©mente un pipeline RAG complet en <strong>3 √©tapes simples</strong> :
                    </p>
                    
                    <div class="workflow-steps">
                        <div class="workflow-step">
                            <div class="workflow-number">1Ô∏è‚É£</div>
                            <div class="workflow-details">
                                <h4>Upload de Documents</h4>
                                <p>
                                    Vous uploadez vos documents (PDF, TXT, DOC, DOCX, Markdown). 
                                    L'application extrait le texte et le stocke localement.
                                </p>
                            </div>
                        </div>
                        
                        <div class="workflow-step">
                            <div class="workflow-number">2Ô∏è‚É£</div>
                            <div class="workflow-details">
                                <h4>Indexation</h4>
                                <p>
                                    Le texte est d√©coup√© en <strong>chunks</strong> (morceaux de 500 tokens avec chevauchement). 
                                    Chaque chunk est transform√© en <strong>embedding via OpenAI</strong>, puis index√© dans 
                                    <strong>FAISS</strong> pour permettre une recherche rapide.
                                </p>
                            </div>
                        </div>
                        
                        <div class="workflow-step">
                            <div class="workflow-number">3Ô∏è‚É£</div>
                            <div class="workflow-details">
                                <h4>Recherche & G√©n√©ration</h4>
                                <p>
                                    Vous posez une question. L'application la transforme en vecteur, 
                                    <strong>retrouve les chunks pertinents</strong> via FAISS, puis utilise 
                                    <strong>GPT-4o-mini</strong> pour g√©n√©rer une r√©ponse bas√©e sur ces sources. 
                                    Les sources utilis√©es sont affich√©es pour garantir la tra√ßabilit√©.
                                </p>
                            </div>
                        </div>
                    </div>
                    
                    <p style="margin-top: 20px;">
                        <strong>üéØ R√©sultat :</strong> Des r√©ponses pr√©cises et contextuelles bas√©es sur vos propres documents, 
                        avec une r√©duction drastique des hallucinations gr√¢ce au grounding sur vos sources.
                    </p>
                </div>
            </section>

            <section class="features-section" id="objectifs">
                <h3>üéØ Ce que vous allez apprendre</h3>
                <div class="features-grid">
                    <div class="feature-card">
                        <div class="feature-icon">üìö</div>
                        <h4>Chunking & Vectorisation</h4>
                        <p>D√©couper et transformer vos documents en vecteurs pour une recherche efficace</p>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üîç</div>
                        <h4>Retrieval Intelligent</h4>
                        <p>Interroger votre corpus avec FAISS pour r√©cup√©rer les documents pertinents</p>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">‚úçÔ∏è</div>
                        <h4>G√©n√©ration de Tests</h4>
                        <p>Cr√©er automatiquement des cas de test √† partir de vos sp√©cifications</p>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üìä</div>
                        <h4>Rapports QA</h4>
                        <p>G√©n√©rer des rapports de qualit√© structur√©s et pertinents</p>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üéì</div>
                        <h4>R√©duction des Hallucinations</h4>
                        <p>Techniques pour garantir la qualit√© et la fiabilit√© des r√©sultats</p>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üö¢</div>
                        <h4>Industrialisation</h4>
                        <p>Pistes pour int√©grer votre solution en production (CI/CD, gouvernance)</p>
                    </div>
                </div>
            </section>

            <section class="tech-stack" id="stack">
                <h3>üõ†Ô∏è Stack Technique</h3>
                <div class="tech-items">
                    <span class="tech-badge">Python</span>
                    <span class="tech-badge">OpenAI Embeddings</span>
                    <span class="tech-badge">FAISS</span>
                    <span class="tech-badge">VS Code</span>
                    <span class="tech-badge">Flask</span>
                </div>
            </section>

            <section class="prerequisites-section" id="prerequis">
                <h3>‚úÖ Pr√©requis pour cet Atelier</h3>
                <div class="prerequisites-content">
                    <div class="prerequisite-category">
                        <h4>üêç Environnement Python</h4>
                        <ul>
                            <li><strong>Python 3.8+</strong> install√© sur votre machine</li>
                            <li>Connaissance de base de Python (variables, fonctions, boucles)</li>
                            <li>Capacit√© √† cr√©er et activer un environnement virtuel (venv)</li>
                        </ul>
                    </div>

                    <div class="prerequisite-category">
                        <h4>üíª Outils de D√©veloppement</h4>
                        <ul>
                            <li><strong>VS Code</strong> ou un √©diteur de code similaire</li>
                            <li>Terminal/ligne de commande (cmd, PowerShell, bash)</li>
                            <li>Git (optionnel, pour cloner le projet)</li>
                        </ul>
                    </div>

                    <div class="prerequisite-category">
                        <h4>üîë Acc√®s OpenAI</h4>
                        <ul>
                            <li><strong>Cl√© API OpenAI</strong> fournie pour cet atelier</li>
                            <li>La cl√© API sera configur√©e dans le fichier <code>.env</code></li>
                            <li>Vous pouvez √©galement utiliser votre propre cl√© si vous le souhaitez</li>
                        </ul>
                    </div>

                    <div class="prerequisite-category">
                        <h4>üì¶ D√©pendances Python</h4>
                        <ul>
                            <li><code>flask</code> - Framework web</li>
                            <li><code>openai</code> - Client OpenAI officiel</li>
                            <li><code>faiss-cpu</code> - Recherche de similarit√© vectorielle</li>
                            <li><code>python-dotenv</code> - Gestion des variables d'environnement</li>
                            <li><code>PyPDF2</code>, <code>python-docx</code>, <code>markdown</code> - Extraction de texte</li>
                            <li><code>tiktoken</code> - Comptage de tokens</li>
                            <li><code>sentence-transformers</code> - Embeddings locaux (optionnel)</li>
                            <li><code>ollama</code> - LLM locaux (optionnel)</li>
                            <li><code>torch</code> - Support pour les mod√®les locaux (optionnel)</li>
                        </ul>
                        <p class="install-note">
                            üí° <strong>Installation automatique :</strong> Un fichier <code>requirements.txt</code> 
                            est fourni pour installer toutes les d√©pendances avec 
                            <code>pip install -r requirements.txt</code>
                        </p>
                    </div>

                    <div class="prerequisite-category">
                        <h4>üìö Connaissances Recommand√©es</h4>
                        <ul>
                            <li>Notions de base en <strong>Machine Learning</strong> (utile mais non obligatoire)</li>
                            <li>Compr√©hension des concepts de <strong>NLP</strong> (Natural Language Processing)</li>
                            <li>Familiarit√© avec les <strong>API REST</strong></li>
                            <li>Int√©r√™t pour le <strong>testing automatis√©</strong> et la QA</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="privacy-section" id="confidentialite">
                <h3>üîí Confidentialit√© et Donn√©es</h3>
                <div class="privacy-content">
                    <div class="privacy-warning">
                        <h4>‚ö†Ô∏è Important : Ce qui est envoy√© √† OpenAI</h4>
                        <p>
                            Dans la configuration actuelle, <strong>vos documents sont envoy√©s √† OpenAI</strong> 
                            lors de l'indexation et de la recherche :
                        </p>
                        <ul>
                            <li><strong>√Ä l'indexation :</strong> Tout le contenu de vos documents (d√©coup√© en chunks) est envoy√© pour cr√©er les embeddings</li>
                            <li><strong>√Ä la recherche :</strong> Votre question et les chunks pertinents r√©cup√©r√©s sont envoy√©s au mod√®le GPT pour g√©n√©rer la r√©ponse</li>
                        </ul>
                    </div>

                    <div class="privacy-policy">
                        <h4>üìã Politique OpenAI</h4>
                        <ul>
                            <li>‚úÖ Les donn√©es API <strong>ne sont PAS utilis√©es pour entra√Æner les mod√®les</strong> (depuis mars 2023)</li>
                            <li>‚úÖ R√©tention : <strong>30 jours</strong> pour monitoring d'abus, puis suppression automatique</li>
                            <li>‚ùå Les donn√©es <strong>transitent par les serveurs OpenAI</strong></li>
                            <li>‚ùå <strong>Non recommand√©</strong> pour des donn√©es sensibles ou confidentielles</li>
                        </ul>
                    </div>

                    <div class="local-data">
                        <h4>üè† Ce qui reste local</h4>
                        <ul>
                            <li>‚úÖ <strong>Index FAISS</strong> (<code>faiss_index.bin</code>) - stock√© sur votre machine</li>
                            <li>‚úÖ <strong>M√©tadonn√©es</strong> (<code>index_metadata.pkl</code>) - stock√© localement</li>
                            <li>‚úÖ <strong>Documents originaux</strong> (dossier <code>uploads/</code>) - restent locaux</li>
                            <li>‚úÖ <strong>Recherche de similarit√©</strong> (FAISS) - calcul en local</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="local-mode-section" id="local">
                <h3>üè° Passer en Mode 100% Local (Sans OpenAI)</h3>
                <div class="local-mode-content">
                    <div class="local-intro">
                        <p>
                            Pour garantir une <strong>confidentialit√© totale</strong>, vous pouvez utiliser des mod√®les 
                            locaux qui ne transmettent <strong>aucune donn√©e</strong> √† des serveurs externes. 
                            Vos documents restent enti√®rement sur votre machine.
                        </p>
                    </div>

                    <div class="local-stack">
                        <h4>üõ†Ô∏è Stack Locale</h4>
                        <div class="tech-items">
                            <span class="tech-badge local">Sentence Transformers</span>
                            <span class="tech-badge local">FAISS</span>
                            <span class="tech-badge local">Ollama / Llama</span>
                        </div>
                        <p style="margin-top: 15px; color: #555;">
                            <strong>Sentence Transformers</strong> pour les embeddings locaux ‚Üí 
                            <strong>FAISS</strong> pour l'indexation ‚Üí 
                            <strong>Ollama</strong> pour la g√©n√©ration de r√©ponses
                        </p>
                    </div>

                    <div class="setup-steps">
                        <h4>üìã √âtapes de Configuration (une seule fois)</h4>
                        
                        <div class="setup-step">
                            <div class="setup-number">1</div>
                            <div class="setup-content">
                                <h5>Installer les d√©pendances Python</h5>
                                <div class="code-block">
                                    <code>pip install sentence-transformers ollama</code>
                                </div>
                                <p>Temps : ~2 minutes | Taille : ~500 MB</p>
                            </div>
                        </div>

                        <div class="setup-step">
                            <div class="setup-number">2</div>
                            <div class="setup-content">
                                <h5>Installer Ollama</h5>
                                <ul>
                                    <li><strong>Windows :</strong> T√©l√©charger l'installeur depuis <a href="https://ollama.com" target="_blank">ollama.com</a></li>
                                    <li><strong>Mac/Linux :</strong> <code>curl -fsSL https://ollama.com/install.sh | sh</code></li>
                                </ul>
                                <p>Temps : ~2 minutes | Taille : ~500 MB</p>
                            </div>
                        </div>

                        <div class="setup-step">
                            <div class="setup-number">3</div>
                            <div class="setup-content">
                                <h5>T√©l√©charger un mod√®le LLM</h5>
                                <div class="code-block">
                                    <code>ollama pull llama3.2:3b</code>
                                    <span class="code-comment"># Mod√®le l√©ger (1.9 GB)</span>
                                </div>
                                <div class="code-block" style="margin-top: 8px;">
                                    <code>ollama pull mistral:7b</code>
                                    <span class="code-comment"># Plus performant (4.1 GB)</span>
                                </div>
                                <p>Temps : ~5-10 minutes | Taille : 2-4 GB</p>
                            </div>
                        </div>

                        <div class="setup-step">
                            <div class="setup-number">4</div>
                            <div class="setup-content">
                                <h5>Configurer le mode local</h5>
                                <p>Dans le fichier <code>.env</code>, ajouter :</p>
                                <div class="code-block">
                                    <code>EMBEDDING_MODE=local</code><br>
                                    <code>LLM_MODE=local</code><br>
                                    <code>OLLAMA_MODEL=llama3.2:3b</code>
                                </div>
                                <p>Temps : ~30 secondes</p>
                            </div>
                        </div>

                        <div class="setup-step">
                            <div class="setup-number">5</div>
                            <div class="setup-content">
                                <h5>Lancer l'application</h5>
                                <div class="code-block">
                                    <code>flask run</code>
                                </div>
                                <p>‚úÖ <strong>C'est tout !</strong> Ollama se lance automatiquement en arri√®re-plan. 
                                   Au premier lancement, Sentence Transformers t√©l√©chargera son mod√®le (~80 MB).</p>
                            </div>
                        </div>
                    </div>

                    <div class="setup-summary">
                        <h4>‚è±Ô∏è R√©sum√© du Setup Initial</h4>
                        <table class="summary-table">
                            <tr>
                                <th>√âtape</th>
                                <th>Temps</th>
                                <th>Taille</th>
                            </tr>
                            <tr>
                                <td>D√©pendances Python</td>
                                <td>~2 min</td>
                                <td>~500 MB</td>
                            </tr>
                            <tr>
                                <td>Installer Ollama</td>
                                <td>~2 min</td>
                                <td>~500 MB</td>
                            </tr>
                            <tr>
                                <td>T√©l√©charger mod√®le LLM</td>
                                <td>~5-10 min</td>
                                <td>2-4 GB</td>
                            </tr>
                            <tr>
                                <td>Premier lancement (embeddings)</td>
                                <td>~30 sec</td>
                                <td>~80 MB</td>
                            </tr>
                            <tr class="total-row">
                                <td><strong>TOTAL</strong></td>
                                <td><strong>~10-15 min</strong></td>
                                <td><strong>~3-5 GB</strong></td>
                            </tr>
                        </table>
                        <p class="summary-note">
                            ‚ö†Ô∏è <strong>Apr√®s le setup initial</strong>, un simple <code>flask run</code> suffit, 
                            comme avec OpenAI !
                        </p>
                    </div>

                    <div class="comparison">
                        <h4>‚öñÔ∏è Comparaison OpenAI vs Local</h4>
                        <table class="comparison-table">
                            <tr>
                                <th>Crit√®re</th>
                                <th>OpenAI</th>
                                <th>100% Local</th>
                            </tr>
                            <tr>
                                <td><strong>Performance</strong></td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s bonne</td>
                            </tr>
                            <tr>
                                <td><strong>Co√ªt</strong></td>
                                <td>‚ùå Payant (API)</td>
                                <td>‚úÖ Gratuit</td>
                            </tr>
                            <tr>
                                <td><strong>Confidentialit√©</strong></td>
                                <td>‚ö†Ô∏è Donn√©es envoy√©es au cloud</td>
                                <td>‚úÖ 100% priv√©</td>
                            </tr>
                            <tr>
                                <td><strong>Configuration</strong></td>
                                <td>‚úÖ Simple (cl√© API)</td>
                                <td>‚ö†Ô∏è Setup initial requis</td>
                            </tr>
                            <tr>
                                <td><strong>Ressources</strong></td>
                                <td>‚úÖ Faibles</td>
                                <td>‚ö†Ô∏è GPU recommand√©</td>
                            </tr>
                            <tr>
                                <td><strong>Connexion Internet</strong></td>
                                <td>‚ùå Obligatoire</td>
                                <td>‚úÖ Fonctionne offline</td>
                            </tr>
                        </table>
                    </div>

                    <div class="recommendation">
                        <h4>üéØ Recommandation</h4>
                        <p>
                            <strong>Pour l'atelier :</strong> Mode OpenAI (simplicit√©, performances optimales)<br>
                            <strong>Pour la production avec donn√©es sensibles :</strong> Mode 100% Local (confidentialit√© garantie)<br>
                            <strong>Id√©al :</strong> Application avec mode dual configurable selon vos besoins
                        </p>
                    </div>
                </div>
            </section>

            
        </main>

        <footer>
            <p>¬© 2025 CFTL JTIA - Tutoriel Mini-RAG | Paris</p>
        </footer>
    </div>
</body>
</html>
